{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leboncoin_ads=pd.read_json(os.getcwd()+\"/leboncoin_ads_batch_1_100.json\")\n",
    "leboncoin_ads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leboncoin_ads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leboncoin_ads.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compter le nombre de modalités par column\n",
    "for column_name in leboncoin_ads.columns:\n",
    "    try:\n",
    "        print(column_name,\" : \",len(set(leboncoin_ads[column_name])))\n",
    "    except TypeError as e:\n",
    "        print(column_name, f\"Error: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First selection of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Sélectionner variables pertinentes:\n",
    "# - Index_date=first_publication_date (retirer doublons)\n",
    "# - Retirer variables avec une seule modalité\n",
    "# - Retirer \"subject\" car les infos sont déjà présentes dans \"attributes\"\n",
    "# -\"similar\" ne contient que des NaN\n",
    "# -\"counters\" ne contient que des dictionnaires vides\n",
    "# -retirer url de l'annonce\n",
    "# -price_cents=price*100\n",
    "leboncoin_ads_filtered=leboncoin_ads[['list_id', 'first_publication_date', 'expiration_date', \n",
    "       'ad_type', 'price', 'images', 'attributes',\n",
    "       'location', 'owner', 'options', 'has_phone',\n",
    "       'is_boosted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processing:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def list_dict_handling(self):\n",
    "        self.loc[:, \"price\"]=self[\"price\"].apply(lambda prix: prix[0])\n",
    "        \"\"\" shatter the variables containing lists and dictionnairies and transform their values into columns\"\"\"\n",
    "        ploo=pd.DataFrame()\n",
    "        attributs=pd.DataFrame()\n",
    "        for row_num in range(self.shape[0]):\n",
    "            ploo_col=pd.DataFrame()\n",
    "            for column_name in [\"location\",'owner','options']:\n",
    "    # Extract 'attributes' column as a DataFrame\n",
    "                temporary_df=pd.DataFrame(self.loc[row_num,column_name].items()).set_index(0).transpose()\n",
    "                ploo_col = pd.concat([ploo_col,temporary_df],axis=1)\n",
    "                ploo_col[\"list_id\"]=self.loc[row_num,\"list_id\"]\n",
    "            ploo=pd.concat([ploo,ploo_col],axis=0,ignore_index=True)\n",
    "            temporary_df=pd.DataFrame(self.loc[row_num,\"attributes\"]).set_index(\"key\").transpose()\n",
    "            temporary_df=temporary_df[temporary_df.index==\"value_label\"]\n",
    "            temporary_df[\"list_id\"]=self.loc[row_num,\"list_id\"]\n",
    "            attributs=pd.concat([attributs,temporary_df],axis=0,ignore_index=True)\n",
    "        extended_dataset=self.merge(pd.concat([attributs,ploo],axis=1).T.reset_index().drop_duplicates(subset=[\"index\"]).set_index('index').T,\n",
    "                                                             how='outer',on='list_id') \n",
    "        return extended_dataset\n",
    "    def library(self):\n",
    "        return pd.DataFrame(self.loc[0][\"attributes\"])[[\"key\",\"key_label\"]]\n",
    "    def clean_columns(self):\n",
    "    #is_boosted : remplacer les NaN par 0\n",
    "        self[\"is_boosted\"]=self[\"is_boosted\"].apply(lambda x: 0 if math.isnan(x) else x)\n",
    "        cols=[\"images\",\"profile_picture_url\",\"vehicle_is_eligible_p2p\",\"car_rotation_delay\",\"licence_plate_available\",\n",
    "      \"vehicle_history_report_public_url\",\"vehicle_damage\",\"activity_sector\",\"vehicle_history_report_status\",\"store_logo\",\"store_name\"]\n",
    "        self[cols]=self[cols].apply(lambda col: col.map(lambda x: 0 if pd.isna(x) else 1))\n",
    "# convertir en numérique\n",
    "        self[\"mileage\"]=self[\"mileage\"].apply(lambda x: float(x.split(\" km\")[0]))\n",
    "        def sparse_years(x):\n",
    "            if pd.notna(x):\n",
    "                if ' ans' in x:\n",
    "                    return float(x.split(' ans')[0])\n",
    "                elif ' an' in x:\n",
    "                    return float(x.split(' an')[0])\n",
    "                else:\n",
    "                    return np.nan\n",
    "            else:\n",
    "                return np.nan\n",
    "        self[\"spare_parts_availability\"]=self[\"spare_parts_availability\"].apply(lambda x:sparse_years(x))\n",
    "        self[\"horsepower\"]=self[\"horsepower\"].apply(lambda x:float(x.split(' Cv')[0]) if pd.notna(x) else np.nan)\n",
    "        self[\"horse_power_din\"]=self[\"horse_power_din\"].apply(lambda x:float(x.split(' Ch')[0]) if pd.notna(x) else np.nan)\n",
    "        cols=['rating_score', 'car_price_min', 'car_price_max',\"old_price\",\"siren\",\"price\",\"lat\",\"lng\",\"regdate\",\"gross_vehicle_weight\"]\n",
    "        self[cols]=self[cols].apply(lambda col: col.map(lambda x: float(x) if pd.notna(x) else np.nan))\n",
    "#replace Nan par \"non spécifé\"\n",
    "        cols=[\"doors\",\"seats\",\"vehicle_technical_inspection_a\",\"vehicle_type\",\"vehicle_upholstery\",\"vehicle_specifications\",\n",
    "      \"vehicule_color\",\"critair\",\"ad_warranty_type\",\"recent_used_vehicle\",\"vehicle_history_report_status\",\n",
    "        \"vehicle_euro_emissions_standard\",'u_car_brand', 'u_car_model',\"car_contract\"]\n",
    "        self[cols]=self[cols].apply(lambda col: col.map (lambda x: \"unknown\" if pd.isna(x) else x))\n",
    "        self[\"vehicle_interior_specs\"]=self[\"vehicle_interior_specs\"].apply(lambda x: \"unknow interior\" if pd.isna(x) else x)\n",
    "#renommer certaines modalités:\n",
    "        self[\"vehicle_interior_specs\"]=self[\"vehicle_interior_specs\"].apply(lambda l: [ \n",
    "        'Phares LED' if x == 'Phares LED / Xenon' \n",
    "        else 'Toit ouvrant' if x == 'Toit ouvrant / Toit panoramique' \n",
    "        else x\n",
    "        for x in l.split(\", \")] if isinstance(l,list) else l)\n",
    "        for spec in [\"Climatisation\", \"Toit ouvrant\",  \"Régulateur de vitesse\", \"Système de navigation\", \"Attelage de remorque\", \n",
    "             \"Aide au stationnement\", \"Jantes en alliage\", \"Bluetooth\", \"Phares LED\", \"Sièges chauffants\",\"unknow interior\"]:\n",
    "            self[spec]=self['vehicle_interior_specs'].apply(lambda x: 1 if spec in x else 0)\n",
    "        return self\n",
    "    def drop_columns(self):\n",
    "        cols=[\"attributes\",\"location\",\"owner\",\"options\",\"vehicle_interior_specs\",# Enlever les variables contenant les dictionnaires\n",
    "              \"brand\",\"model\",\"rating_count\",\"region_id\",\"department_id\",\"city_label\",#Enlever variables répétitives\n",
    "              \"argus_object_id\",\"feature\",\"list_id\",'online_store_id','custom_ref','store_id','user_id',#Enlever variables non pertinentes\n",
    "              \"vehicle_vsp\",\"is_import\",\"country_id\",\"provider\",\"booster\",'photosup', 'urgent', 'gallery','continuous_top_ads','highlight',\n",
    "              \"ad_type\",\"images\"#Enlever variables avec une seule modalité:\n",
    "              ]\n",
    "        return self.drop(columns=cols)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Processing\n",
    "leboncoin_ads_filtered_extended=data_processing.list_dict_handling(leboncoin_ads_filtered)\n",
    "dico_attributs=data_processing.library(leboncoin_ads_filtered)\n",
    "leboncoin_ads_filtered_extended=data_processing.clean_columns(leboncoin_ads_filtered_extended)\n",
    "leboncoin_ads_filtered_extended=data_processing.drop_columns(leboncoin_ads_filtered_extended)\n",
    "leboncoin_ads_filtered_extended=leboncoin_ads_filtered_extended.rename(columns={\"mileage\":\"mileage_km\",\n",
    "                                                                                \"horsepower\":\"horsepower_cv\"})\n",
    "print(\"Taille:\",leboncoin_ads_filtered_extended.shape,\"\\ncolumns: \",leboncoin_ads_filtered_extended.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to datetime\n",
    "leboncoin_ads_filtered_extended[\"first_publication_date\"] = pd.to_datetime(leboncoin_ads_filtered_extended[\"first_publication_date\"])\n",
    "leboncoin_ads_filtered_extended[\"expiration_date\"] = pd.to_datetime(leboncoin_ads_filtered_extended[\"expiration_date\"])\n",
    "\n",
    "#Today\n",
    "today = datetime.today()\n",
    "# Calculate age of ad in days\n",
    "leboncoin_ads_filtered_extended[\"age_annonce(in days)\"] = leboncoin_ads_filtered_extended[\"first_publication_date\"].apply(\n",
    "    lambda date: (\n",
    "        relativedelta(today, date).years * 365 +\n",
    "        relativedelta(today, date).months * 30 +\n",
    "        relativedelta(today, date).days\n",
    "    ) if pd.notna(date) else np.nan  # Return NaN if date is NaT\n",
    ")\n",
    "\n",
    "# Calculate delay of ad in days\n",
    "leboncoin_ads_filtered_extended[\"delai_annonce(in days)\"] = leboncoin_ads_filtered_extended.apply(\n",
    "    lambda row: (\n",
    "        relativedelta(row[\"expiration_date\"], row[\"first_publication_date\"]).years * 365 +\n",
    "        relativedelta(row[\"expiration_date\"], row[\"first_publication_date\"]).months * 30 +\n",
    "        relativedelta(row[\"expiration_date\"], row[\"first_publication_date\"]).days\n",
    "    ) if pd.notna(row[\"first_publication_date\"]) and pd.notna(row[\"expiration_date\"]) else np.nan,  # Return NaN if either date is NaT\n",
    "    axis=1\n",
    ")\n",
    "#Calculate age model of the car\n",
    "leboncoin_ads_filtered_extended[\"age_model_car\"]=leboncoin_ads_filtered_extended[\"regdate\"].apply(lambda date: 2024-date)\n",
    "\n",
    "#Delete \"engineered\" columns\n",
    "leboncoin_ads_filtered_extended.drop(columns=[\"first_publication_date\",\"expiration_date\",\"regdate\"],inplace=True)\n",
    "print(\"Taille:\",leboncoin_ads_filtered_extended.shape,\"\\ncolumns: \",leboncoin_ads_filtered_extended.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price distribution:\n",
    "leboncoin_ads_filtered_extended.describe()[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical variables correlation\n",
    "leboncoin_ads_filtered_extended[leboncoin_ads_filtered_extended.columns[~leboncoin_ads_filtered_extended.columns.isin(leboncoin_ads_filtered_extended.select_dtypes(include='object').columns)]].corr()[\"price\"] \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation avec variables catégorielles\n",
    "#Correlation avec la demande de places de parkings\n",
    "for col in leboncoin_ads_filtered_extended.select_dtypes(include='object').columns:\n",
    "        track=0\n",
    "        for modalite1 in set(list(leboncoin_ads_filtered_extended[col])):\n",
    "            for modalite2 in set(list(leboncoin_ads_filtered_extended[col])):\n",
    "                if track==1:\n",
    "                    break\n",
    "                if modalite1!=modalite2:\n",
    "                    try: \n",
    "                        if kruskal(leboncoin_ads_filtered_extended[leboncoin_ads_filtered_extended[col]==modalite1]['price'],\n",
    "                            leboncoin_ads_filtered_extended[leboncoin_ads_filtered_extended[col]==modalite2]['price']).pvalue<0.05:\n",
    "                            track+=1\n",
    "                            print(col,\"Reject the null hypothesis: There is a significant difference between the groups.\")\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of missing information\n",
    "proportion_missing_factors={}\n",
    "\n",
    "for col in ['is_boosted',\"profile_picture_url\",\"vehicle_is_eligible_p2p\",\"car_rotation_delay\",\"licence_plate_available\",\n",
    "      \"vehicle_history_report_public_url\",\"vehicle_damage\",\"activity_sector\",\"vehicle_history_report_status\"]:\n",
    "    proportions=leboncoin_ads_filtered_extended[col].value_counts(normalize=True)\n",
    "    proportion_missing_factors[col]=proportions.get(0,0)\n",
    "for col in [\"doors\",\"seats\",\"vehicle_technical_inspection_a\",\"vehicle_type\",\"vehicle_upholstery\",\"vehicle_specifications\",\n",
    "      \"vehicule_color\",\"critair\",\"horsepower_cv\",\"ad_warranty_type\",\"recent_used_vehicle\",\"vehicle_history_report_status\",\n",
    "        \"spare_parts_availability\",\"vehicle_euro_emissions_standard\"]:\n",
    "    proportions=(leboncoin_ads_filtered_extended[col].value_counts()/leboncoin_ads_filtered_extended.shape[0])\n",
    "    proportion_missing_factors[col]=proportions.get(\"unknwonw\", 0)\n",
    "for col in ['rating_score', 'car_price_min', 'car_price_max',\"old_price\",\"siren\",\"price\",\"lat\",\"lng\",\"horsepower_cv\",\"mileage_km\"]:\n",
    "    proportion_missing_factors[col]=leboncoin_ads_filtered_extended[col].isna().sum()/leboncoin_ads_filtered_extended.shape[0]\n",
    "proportion_missing_factors[\"unknow interior\"]=leboncoin_ads_filtered_extended[\"unknow interior\"].value_counts(normalize=True).get(1,0)\n",
    "#afficher facteurs informations non disponibles\n",
    "{key: value for key, value in proportion_missing_factors.items() if value > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colonnes necessaires à explorer (d'après littérature et analyse de correlation (seuil 20%)):\n",
    "cols_to_explore_num=['mileage_km',\"age_model_car\",\"gross_vehicle_weight\",\"horsepower_cv\",\"horse_power_din\"]#variables numériques\n",
    "cols_to_explore_bin=['vehicle_history_report_public_url',\"profile_picture_url\",\"activity_sector\",\"vehicle_is_eligible_p2p\"] #variables binaires\n",
    "cols_to_explore_qual=[ 'doors',\"seats\",'region_name','u_car_brand','fuel','gearbox','vehicle_type','vehicule_color'\n",
    "                      ,\"ad_warranty_type\"]#Variables qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "def binary_categorical_plot(column_name, data):\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot the boxplot of price based on the binary variable\n",
    "    if len(data[column_name].unique())<=4:\n",
    "        sns.boxplot(x=column_name, y='price', data=data, ax=ax)\n",
    "        ax.set_title(f'Price Distribution by {column_name}')\n",
    "        ax.set_xlabel(f'{column_name}')\n",
    "        ax.set_ylabel('Price')\n",
    "    else:\n",
    "        data=leboncoin_ads_filtered_extended.groupby(column_name)['price'].mean().reset_index()\n",
    "        sns.barplot(x=column_name,y=\"price\", data=data)\n",
    "        ax.set_title(f'Mean price pistribution by {column_name}')\n",
    "        ax.set_xlabel(f'{column_name}')\n",
    "        ax.set_ylabel('Price')\n",
    "\n",
    "    # Calculate proportions of each modality in the binary variable\n",
    "    count_data = data[column_name].value_counts(normalize=True)\n",
    "    \n",
    "    # Set the x-ticks to display the proportions\n",
    "    labels = [f'{label} ({count_data[label]:.2%})' for label in count_data.index]\n",
    "    ticks=[i for i in range(len(labels))]\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels, rotation=90)  # Rotate labels to 90 degrees\n",
    "\n",
    "    # Add mean price value on top of each boxplot\n",
    "    mean_price = data.groupby(column_name)['price'].mean()\n",
    "    for i, label in enumerate(count_data.index):\n",
    "        ax.text(i, mean_price[label], f'{mean_price[label]:.0f}', \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "    \n",
    "    #save graphs\n",
    "    save_path = os.path.join(\"graphs\", f'Price Distribution by {column_name}.png')\n",
    "    plt.savefig(save_path)\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_to_explore_bin + cols_to_explore_qual:\n",
    "    print(binary_categorical_plot(col,leboncoin_ads_filtered_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "def num_plot(column_name,data):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    sns.boxplot( y=column_name, data=data, ax=ax1)\n",
    "    ax1.set_title(f'Distribution of {column_name}')\n",
    "    ax1.set_ylabel(column_name)\n",
    "    mean_price = data[column_name].mean()\n",
    "    # Add the mean as a horizontal line\n",
    "    ax1.axhline(mean_price, color='red', linestyle='--', label=f'Mean: {mean_price:.0f}')\n",
    "    # Optionally, add text for the mean value\n",
    "    ax1.text(0.05, mean_price, f'Mean: {mean_price:.2f}', ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "    sns.scatterplot(x=col, y='price', data=data)\n",
    "    ax2.set_title(f'{column_name} vs price')\n",
    "    ax2.set_xlabel(column_name)\n",
    "    ax2.set_ylabel('Price')\n",
    "    \n",
    "    #save graphs\n",
    "    save_path = os.path.join(\"graphs\", f'Price Distribution by {column_name}.png')\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_to_explore_num:\n",
    "    print(num_plot(col,leboncoin_ads_filtered_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=leboncoin_ads_filtered_extended.groupby('horse_power_din')['price'].mean().reset_index()\n",
    "# sns.scatterplot(x='horse_power_din',y=\"price\", data=data)\n",
    "# for i in range(len(data)):\n",
    "#     plt.text(data['horse_power_din'][i], data['price'][i], \n",
    "#              f'{data[\"price\"][i]:.2f}',  # Format the price value\n",
    "#              ha='center', va='bottom', fontsize=10, color='black')\n",
    "#     plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80, 6))\n",
    "ax=sns.heatmap(leboncoin_ads_filtered_extended[leboncoin_ads_filtered_extended.columns[~leboncoin_ads_filtered_extended.columns.isin(leboncoin_ads_filtered_extended.select_dtypes(include='object').columns)]].corr()[['price']].transpose() , \n",
    "               annot=True, \n",
    "               cmap=sns.cubehelix_palette(as_cmap=True),\n",
    "               vmin=-1, vmax=1, annot_kws={\"size\": 30})\n",
    "plt.tick_params(axis='both', which='major', labelsize=30)\n",
    "save_path = os.path.join(\"graphs\", 'correlation_var_num_with_price.png')\n",
    "plt.savefig(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target vs variables\n",
    "y = leboncoin_ads_filtered_extended[\"price\"]\n",
    "X = leboncoin_ads_filtered_extended.drop([\"price\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features=leboncoin_ads_filtered_extended.columns[~leboncoin_ads_filtered_extended.columns.isin(leboncoin_ads_filtered_extended.select_dtypes(include='object').columns)]\n",
    "categorical_features=leboncoin_ads_filtered_extended.columns[leboncoin_ads_filtered_extended.columns.isin(leboncoin_ads_filtered_extended.select_dtypes(include='object').columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, random_state = 0,test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),  # Replace with your numerical columns\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # Replace with your categorical columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Models and their parameter grids\n",
    "models = [\n",
    "    ('linear_reg', LinearRegression()),  # Base model\n",
    "    ('decision_tree', DecisionTreeRegressor(random_state=42)),\n",
    "    ('random_forest', RandomForestRegressor(random_state=42)),\n",
    "    ('neural_net', MLPRegressor(max_iter=1000, random_state=42)),\n",
    "    ('xgboost', XGBRegressor(random_state=42)),\n",
    "    ('catboost', CatBoostRegressor(verbose=0, random_state=42)),\n",
    "    ('lightgbm', LGBMRegressor(random_state=42))\n",
    "]\n",
    "\n",
    "param_grids = {\n",
    "    'linear_reg': {},\n",
    "    'decision_tree': {\n",
    "        'classifier__max_depth': [None]+[i for i in range(2,11,2)],\n",
    "        'classifier__min_samples_split': [i for i in range (2,11,2)],\n",
    "        'classifier__min_samples_leaf': [i for i in range (2,11,2)]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'classifier__n_estimators': [i for i in range(50, 200, 25)],\n",
    "        'classifier__max_depth': [None]+[i for i in range(2,11,2)],\n",
    "        'classifier__min_samples_split': [i for i in range (2,11,2)],\n",
    "        'classifier__min_samples_leaf': [i for i in range (2,11,2)]\n",
    "    },\n",
    "    'neural_net': {\n",
    "        'classifier__activation':['identity','relu','tanh','logistic'],\n",
    "        'classifier__learning_rate':[i/10 for i in range(1,6)],\n",
    "        'classifier__max_iter': [i for i in range(50, 200, 50)],\n",
    "        'classifier__alpha': [0.0001, 0.001, 0.01]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'classifier__n_estimators': [i for i in range(50, 200, 25)],\n",
    "        'classifier__max_depth': [None]+[i for i in range(2,11,2)],\n",
    "        'classifier__min_samples_split': [i for i in range (2,11,2)],\n",
    "        'classifier__min_samples_leaf': [i for i in range (2,11,2)],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'catboost': {\n",
    "        'classifier__depth': [None]+[i for i in range(2,11,5)],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__iterations': [100, 200]\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'classifier__num_leaves': [31, 50, 100],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__n_estimators': [50, 100, 200]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearRegression())  # Placeholder for GridSearchCV\n",
    "])\n",
    "\n",
    "# Define the scoring metric (Mean Squared Error)\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "best_models = {}\n",
    "\n",
    "for name, model in models:\n",
    "    print(f\"Training {name}...\")\n",
    "    # Update pipeline with the model\n",
    "    pipe.set_params(classifier=model)\n",
    "    # Create GridSearchCV object with the model's parameter grid\n",
    "    grid_search = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grids[name] if name in param_grids else {},\n",
    "        cv=3,\n",
    "        scoring=scorer,\n",
    "        refit=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    # Store best model and its score\n",
    "    best_models[name] = {\n",
    "        'best_estimator': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': -grid_search.best_score_  # Convert negative MSE to positive\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for name, result in best_models.items():\n",
    "    print(f\"\\n{name}:\\nBest Params: {result['best_params']}\\nBest MSE: {result['best_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leboncoin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
